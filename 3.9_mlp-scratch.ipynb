{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9 多层感知机的从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") # 为了导入上层目录的d2lzh_pytorch\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.1 获取和读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.2 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层256+\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)\n",
    "\n",
    "# 梯度追踪\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.3 定义激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不使用pytorch的relu，自己用torch.max写\n",
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.4 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.view((-1, num_inputs))\n",
    "    H = relu(torch.matmul(X, W1) + b1)\n",
    "    return torch.matmul(H, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.5 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9.6 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0031, train acc 0.708, test acc 0.753\n",
      "epoch 2, loss 0.0019, train acc 0.823, test acc 0.815\n",
      "epoch 3, loss 0.0017, train acc 0.844, test acc 0.790\n",
      "epoch 4, loss 0.0015, train acc 0.855, test acc 0.834\n",
      "epoch 5, loss 0.0015, train acc 0.864, test acc 0.809\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 100.0\n",
    "# 定义在前几节的函数\n",
    "# def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "#               params=None, lr=None, optimizer=None):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "#         for X, y in train_iter:\n",
    "#             y_hat = net(X)\n",
    "#             l = loss(y_hat, y).sum()\n",
    "#             \n",
    "#             # 梯度清零\n",
    "#             if optimizer is not None:\n",
    "#                 optimizer.zero_grad()\n",
    "#             elif params is not None and params[0].grad is not None:\n",
    "#                 for param in params:\n",
    "#                     param.grad.data.zero_()\n",
    "#            \n",
    "#             l.backward()\n",
    "#             if optimizer is None:\n",
    "#                 d2l.sgd(params, lr, batch_size)\n",
    "#             else:\n",
    "#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到\n",
    "#             \n",
    "#             \n",
    "#             train_l_sum += l.item()\n",
    "#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "#             n += y.shape[0]\n",
    "#         test_acc = evaluate_accuracy(test_iter, net)\n",
    "#         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "注：由于原书的mxnet中的SoftmaxCrossEntropyLoss在反向传播的时候相对于沿batch维求和了，而PyTorch默认的是求平均，所以用PyTorch计算得到的loss比mxnet小很多（大概是maxnet计算得到的1/batch_size这个量级），所以反向传播得到的梯度也小很多，所以为了得到差不多的学习效果，我们把学习率调得成原书的约batch_size倍，原书的学习率为0.5，这里设置成100.0。(之所以这么大，应该是因为d2lzh_pytorch里面的sgd函数在更新的时候除以了batch_size，其实PyTorch在计算loss的时候已经除过一次了，sgd这里应该不用除了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
